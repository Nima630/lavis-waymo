# #  # Copyright (c) 2022, salesforce.com, inc.
# #  # All rights reserved.
# #  # SPDX-License-Identifier: BSD-3-Clause
# #  # For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

# # model:
# #   arch: blip2
# #   model_type: pretrain
# #   load_pretrained: False #pretrain from scratch
# #   freeze_vit: True
# #   model_kwargs:
# #       qformer_config: "blip2_qformer"   # ← this usually points to a file like blip2_qformer.yaml
# #       qformer_pretrained: null          # ← make sure Q-Former is trained from scratch


# # datasets:
# #   coco_caption:
# #     vis_processor:
# #         train:
# #           name: "blip2_image_train"
# #           image_size: 224
# #     text_processor:
# #         train:
# #           name: "blip_caption"
# # #     build_info:
# # #         images:
# # #             storage: '/export/share/datasets/vision/coco/images/'          
# #   vg_caption: # name of the dataset builder
# #     vis_processor:
# #         train:
# #           name: "blip_image_train"
# #           image_size: 224
# #     text_processor:
# #         train:
# #           name: "blip_caption"
# # #     build_info:
# # #         images:
# # #             storage: '//export/share/datasets/vision/visual-genome/image/'

# # run:
# #   task: image_text_pretrain
# #   # optimizer
# #   lr_sched: "linear_warmup_cosine_lr"
# #   init_lr: 1e-4
# #   min_lr: 1e-5
# #   warmup_lr: 1e-6

# #   weight_decay: 0.05
# #   max_epoch: 10
# #   batch_size_train: 100
# #   batch_size_eval: 64
# #   num_workers: 4
# #   warmup_steps: 5000

# #   seed: 42
# #   # output_dir: "output/BLIP2/Pretrain_stage1"
# #   output_dir: "output/BLIP2/qformer_scratch_2gpu"
# #   amp: True
# #   resume_ckpt_path: null

# #   evaluate: False 
# #   train_splits: ["train"]

# #   device: "cuda"
# #   world_size: 1
# #   dist_url: "env://"
# #   distributed: True
  



# # Copyright (c) 2022, salesforce.com, inc.
# # All rights reserved.
# # SPDX-License-Identifier: BSD-3-Clause

# model:
#   arch: blip2
#   model_type: pretrain
#   load_pretrained: False  # pretrain from scratch
#   freeze_vit: True
#   model_kwargs:
#     qformer_config: "blip2_qformer"
#     qformer_pretrained: null

# datasets:
#   coco_caption:
#     vis_processor:
#       train:
#         name: "blip2_image_train"
#         image_size: 224
#     text_processor:
#       train:
#         name: "blip_caption"
#     build_info:
#       annotations:
#         train:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/annotations/coco_karpathy_train.json
#         val:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/annotations/coco_karpathy_val.json
#         test:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/annotations/coco_karpathy_test.json
#       images:
#         storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/images

#   # vg_caption:
#   #   vis_processor:
#   #     train:
#   #       name: "blip_image_train"
#   #       image_size: 224
#   #   text_processor:
#   #     train:
#   #       name: "blip_caption"
#   #   build_info:
#   #     annotations:
#   #       train:
#   #         storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/vg/annotations/vg_caption.json
#   #     images:
#   #       storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/vg/images

# run:
#   task: image_text_pretrain
#   lr_sched: "linear_warmup_cosine_lr"
#   init_lr: 1e-4
#   min_lr: 1e-5
#   warmup_lr: 1e-6
#   weight_decay: 0.05
#   max_epoch: 10
#   # batch_size_train: 100
#   batch_size_train: 32
#   batch_size_eval: 64
#   num_workers: 4
#   warmup_steps: 5000
#   seed: 42
#   output_dir: "output/BLIP2/qformer_scratch_2gpu"
#   amp: True
#   resume_ckpt_path: null
#   evaluate: False
#   train_splits: ["train"]
#   device: "cuda"
#   world_size: 1
#   dist_url: "env://"
#   distributed: True











# model:
#   arch: blip2_pretrain
#   model_type: pretrain_qformer 
#   load_pretrained: False
#   freeze_vit: True
# datasets:
#   coco_caption:
#     data_type: images
#     vis_processor:
#       train:
#         name: "blip2_image_train"
#         image_size: 224
#     text_processor:
#       train:
#         name: "blip_caption"
#     build_info:
#       annotations:
#         train:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/annotations/coco_karpathy_train.json
#           url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json
#         val:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/annotations/coco_karpathy_val.json
#           url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json
#         test:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/annotations/coco_karpathy_test.json
#           url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json
#       images:
#         storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/coco/images




#   vg_caption:
#     data_type: images
#     vis_processor:
#       train:
#         name: "blip_image_train"
#         image_size: 224
#     text_processor:
#       train:
#         name: "blip_caption"
#     build_info:
#       annotations:
#         train:
#           storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/vg/annotations/vg_caption.json
#       images:
#         storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/my_datasets/vg/images
# run:
#   task: image_text_pretrain
#   lr_sched: "linear_warmup_cosine_lr"
#   init_lr: 1e-4
#   min_lr: 1e-5
#   warmup_lr: 1e-6
#   weight_decay: 0.05
#   max_epoch: 10
#   batch_size_train: 32
#   batch_size_eval: 64
#   num_workers: 4
#   warmup_steps: 5000
#   seed: 42
#   output_dir: "output/BLIP2/qformer_scratch_2gpu"
#   amp: true
#   resume_ckpt_path: null
#   evaluate: false
#   train_splits: ["train"]
#   device: "cuda"
#   world_size: 2
#   dist_url: "env://"
#   distributed: true



model:
  arch: blip2
  model_type: pretrain_qformer
  load_pretrained: False
  freeze_vit: True

datasets:
  coco_caption:
    data_type: images
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
    build_info:
      annotations:
        train:
          # storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/coco/annotations/coco_karpathy_train.json
          # url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_train.json
          storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/coco/annotations/coco_tiny_train.json
          url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_tiny_train.json

        val:
          storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/coco/annotations/coco_karpathy_val.json
          url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val.json
        test:
          storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/coco/annotations/coco_karpathy_test.json
          url: https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test.json
      images:
        storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/coco/images

  # vg_caption:
  #   data_type: images
  #   vis_processor:
  #     train:
  #       name: "blip_image_train"
  #       image_size: 224
  #   text_processor:
  #     train:
  #       name: "blip_caption"
  #   build_info:
  #     annotations:
  #       train:
  #         storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/vg/annotations/vg_caption.json
  #     images:
  #       storage: /home/draiman/Desktop/Nima/LAVIS_/LAVIS/lavis/my_datasets/vg/images

run:
  task: image_text_pretrain
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-4
  min_lr: 1e-5
  warmup_lr: 1e-6
  weight_decay: 0.05
  max_epoch: 2 #10 #2
  save_every_n_steps: 2 #1000 #2
  batch_size_train: 2 #16 #2
  batch_size_eval: 64
  num_workers: 4
  warmup_steps: 5 #5000 #5
  seed: 42
  output_dir: "output/BLIP2/qformer_scratch_2gpu"
  amp: true
  resume_ckpt_path: null
  evaluate: false
  train_splits: ["train"]
  device: "cuda"
  world_size: 2
  dist_url: "env://"
  distributed: true



